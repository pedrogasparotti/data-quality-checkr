# Python for Data Engineers

## Core Philosophy

**Build libraries, not scripts.** Mature data engineers package reusable code as libraries with proper testing, versioning, and documentation. This guide focuses on pure Python patterns (no Spark) using modern tools.

---

## Table of Contents

1. [Modern Python Packaging](#modern-python-packaging)
2. [Project Structure](#project-structure)
3. [Core Data Processing Patterns](#core-data-processing-patterns)
4. [File I/O Patterns](#file-io-patterns)
5. [Data Validation & Quality](#data-validation--quality)
6. [Testing Patterns](#testing-patterns)
7. [Performance Optimization](#performance-optimization)
8. [Common Utilities](#common-utilities)

---

## Modern Python Packaging

### The pyproject.toml Era

Modern Python projects use `pyproject.toml` instead of `setup.py`. This is the **only** file you need for most packages.

**Minimal Example:**

```toml
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "my_data_lib"
version = "0.1.0"
description = "Data engineering utilities"
readme = "README.md"
requires-python = ">=3.9"
dependencies = [
    "polars>=0.20.0",
    "duckdb>=0.9.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0",
    "black>=23.0",
    "ruff>=0.1.0",
]

[project.scripts]
process-data = "my_data_lib.cli:main"
```

### Project Layout

```
my_data_lib/
├── pyproject.toml           # Package configuration
├── README.md                # Documentation
├── src/
│   └── my_data_lib/         # Source code
│       ├── __init__.py
│       ├── extractors.py    # Data extraction
│       ├── transformers.py  # Transformation logic
│       ├── loaders.py       # Data loading
│       └── validators.py    # Quality checks
├── tests/
│   ├── __init__.py
│   ├── test_extractors.py
│   ├── test_transformers.py
│   └── fixtures/            # Test data
└── .gitignore
```

### Building & Installing

```bash
# Build the package
python -m build

# Editable install (for development)
pip install -e .

# Install with dev dependencies
pip install -e ".[dev]"

# Install from wheel
pip install dist/my_data_lib-0.1.0-py3-none-any.whl
```

### Publishing to PyPI

```bash
# Install twine
pip install twine

# Upload to TestPyPI first
python -m twine upload --repository testpypi dist/*

# Upload to PyPI
python -m twine upload dist/*
```

---

## Project Structure

### Recommended Architecture

```python
# src/my_data_lib/__init__.py
"""
Data engineering library for processing user events.
"""
__version__ = "0.1.0"

from .extractors import extract_from_postgres, extract_from_s3
from .transformers import clean_user_events, aggregate_metrics
from .loaders import load_to_warehouse

__all__ = [
    "extract_from_postgres",
    "extract_from_s3",
    "clean_user_events",
    "aggregate_metrics",
    "load_to_warehouse",
]
```

### Module Organization

**extractors.py** - Read data from sources
```python
from pathlib import Path
import polars as pl

def extract_from_csv(filepath: Path) -> pl.DataFrame:
    """Extract data from CSV file."""
    return pl.read_csv(filepath)

def extract_from_postgres(
    conn_string: str, 
    query: str
) -> pl.DataFrame:
    """Extract data from Postgres database."""
    import duckdb
    
    con = duckdb.connect()
    con.execute(f"ATTACH '{conn_string}' AS postgres_db (TYPE postgres)")
    return con.execute(query).pl()
```

**transformers.py** - Business logic
```python
import polars as pl
from typing import List

def clean_user_events(df: pl.DataFrame) -> pl.DataFrame:
    """Clean and standardize user event data."""
    return (
        df
        .filter(pl.col("event_timestamp").is_not_null())
        .with_columns([
            pl.col("user_id").cast(pl.Utf8),
            pl.col("event_timestamp").cast(pl.Datetime),
        ])
        .unique(subset=["event_id"])
    )

def aggregate_metrics(
    df: pl.DataFrame, 
    group_cols: List[str]
) -> pl.DataFrame:
    """Aggregate metrics by specified columns."""
    return (
        df
        .group_by(group_cols)
        .agg([
            pl.count().alias("event_count"),
            pl.col("revenue").sum().alias("total_revenue"),
            pl.col("user_id").n_unique().alias("unique_users"),
        ])
    )
```

**loaders.py** - Write data to destinations
```python
import polars as pl
from pathlib import Path

def load_to_parquet(
    df: pl.DataFrame, 
    output_path: Path,
    partition_by: str | None = None
) -> None:
    """Load dataframe to partitioned parquet."""
    if partition_by:
        df.write_parquet(
            output_path,
            partition_by=partition_by,
            compression="snappy"
        )
    else:
        df.write_parquet(output_path, compression="snappy")

def load_to_warehouse(
    df: pl.DataFrame,
    table_name: str,
    conn_string: str,
    mode: str = "append"  # append, replace, fail
) -> None:
    """Load dataframe to data warehouse."""
    import duckdb
    
    con = duckdb.connect(conn_string)
    
    if mode == "replace":
        con.execute(f"DROP TABLE IF EXISTS {table_name}")
    
    con.execute(f"CREATE TABLE IF NOT EXISTS {table_name} AS SELECT * FROM df")
```

---

## Core Data Processing Patterns

### Use the Right Tool

**DuckDB** - For SQL-heavy transformations on medium data (<100GB)
```python
import duckdb

con = duckdb.connect("warehouse.db")

# SQL transformations
result = con.execute("""
    SELECT 
        user_id,
        DATE_TRUNC('day', event_time) as event_date,
        COUNT(*) as event_count
    FROM events
    WHERE event_time >= '2024-01-01'
    GROUP BY 1, 2
""").pl()
```

**Polars** - For dataframe operations, fast and memory-efficient
```python
import polars as pl

df = (
    pl.read_parquet("events.parquet")
    .filter(pl.col("event_time") >= "2024-01-01")
    .group_by(["user_id", pl.col("event_time").dt.date()])
    .agg(pl.count().alias("event_count"))
)
```

**Standard Library** - For simple transformations
```python
from pathlib import Path
from collections import Counter
import json

# Process line-by-line for large files
def count_event_types(filepath: Path) -> dict:
    """Count events without loading entire file."""
    counter = Counter()
    
    with open(filepath) as f:
        for line in f:
            event = json.loads(line)
            counter[event['type']] += 1
    
    return dict(counter)
```

### When to Use What

| Data Size | Processing | Tool Choice |
|-----------|------------|-------------|
| < 1GB | In-memory | Polars, Pandas |
| 1-100GB | Single machine | DuckDB, Polars lazy |
| > 100GB | Distributed | Spark, Snowflake, BigQuery |

---

## File I/O Patterns

### Reading Data

**Parquet (Preferred for analytics)**
```python
import polars as pl

# Read single file
df = pl.read_parquet("data.parquet")

# Read partitioned data
df = pl.scan_parquet("data/**/*.parquet").collect()

# Read specific columns
df = pl.read_parquet("data.parquet", columns=["user_id", "revenue"])
```

**CSV**
```python
# Handle messy CSVs
df = pl.read_csv(
    "messy_data.csv",
    has_header=True,
    skip_rows=2,           # Skip metadata rows
    ignore_errors=True,     # Skip bad rows
    null_values=["NA", "NULL", ""],
    encoding="utf8-lossy"   # Handle encoding issues
)
```

**JSON Lines (JSONL)**
```python
# For log files and event streams
df = pl.read_ndjson("events.jsonl")

# Or with standard library
from pathlib import Path
import json

def read_jsonl(filepath: Path):
    with open(filepath) as f:
        for line in f:
            yield json.loads(line)

# Usage
events = list(read_jsonl(Path("events.jsonl")))
```

### Writing Data

```python
# Parquet with partitioning
df.write_parquet(
    "output/",
    partition_by="event_date",
    compression="snappy"
)

# CSV
df.write_csv("output.csv", separator="|")

# JSON Lines
df.write_ndjson("output.jsonl")
```

### Efficient File Processing

**Stream large files**
```python
from pathlib import Path

def process_large_csv_streaming(
    input_path: Path, 
    output_path: Path,
    chunk_size: int = 100_000
):
    """Process CSV in chunks to avoid memory issues."""
    for chunk in pl.read_csv_batched(input_path, batch_size=chunk_size):
        processed = transform_data(chunk)
        
        # Append to output
        processed.write_parquet(
            output_path,
            mode="append"  # Polars doesn't support this yet
        )
```

**Use lazy execution**
```python
# Don't load data until needed
lazy_df = (
    pl.scan_parquet("large_data.parquet")
    .filter(pl.col("year") == 2024)
    .select(["user_id", "revenue"])
)

# Only executes when you call .collect()
result = lazy_df.collect()
```

---

## Data Validation & Quality

### Schema Validation

```python
import polars as pl
from typing import Dict, Any

def validate_schema(
    df: pl.DataFrame, 
    expected_schema: Dict[str, Any]
) -> None:
    """Validate dataframe matches expected schema."""
    for col_name, expected_type in expected_schema.items():
        if col_name not in df.columns:
            raise ValueError(f"Missing column: {col_name}")
        
        actual_type = df[col_name].dtype
        if actual_type != expected_type:
            raise TypeError(
                f"Column {col_name}: expected {expected_type}, "
                f"got {actual_type}"
            )

# Usage
expected = {
    "user_id": pl.Utf8,
    "event_time": pl.Datetime,
    "revenue": pl.Float64,
}
validate_schema(df, expected)
```

### Data Quality Checks

```python
import polars as pl
from dataclasses import dataclass

@dataclass
class QualityReport:
    """Data quality metrics."""
    total_rows: int
    null_counts: dict
    duplicate_count: int
    passed: bool

def quality_check(
    df: pl.DataFrame, 
    unique_cols: list[str]
) -> QualityReport:
    """Run comprehensive quality checks."""
    
    # Check for nulls
    null_counts = {
        col: df[col].null_count()
        for col in df.columns
    }
    
    # Check for duplicates
    duplicate_count = df.select(unique_cols).is_duplicated().sum()
    
    # Determine if passed
    has_critical_nulls = any(
        count > 0 for col, count in null_counts.items()
        if col in unique_cols
    )
    
    passed = not has_critical_nulls and duplicate_count == 0
    
    return QualityReport(
        total_rows=len(df),
        null_counts=null_counts,
        duplicate_count=duplicate_count,
        passed=passed
    )
```

### Great Expectations Integration

```python
import great_expectations as gx

def validate_with_gx(df: pl.DataFrame) -> bool:
    """Validate data using Great Expectations."""
    
    context = gx.get_context()
    
    # Convert Polars to Pandas (GX doesn't support Polars yet)
    pandas_df = df.to_pandas()
    
    # Create expectations
    validator = context.sources.pandas_default.read_dataframe(pandas_df)
    
    validator.expect_column_values_to_not_be_null("user_id")
    validator.expect_column_values_to_be_unique("event_id")
    validator.expect_column_values_to_be_between(
        "revenue", 
        min_value=0, 
        max_value=10000
    )
    
    # Validate
    results = validator.validate()
    return results.success
```

---

## Testing Patterns

### Unit Testing with Pytest

```python
# tests/test_transformers.py
import polars as pl
import pytest
from my_data_lib.transformers import clean_user_events

@pytest.fixture
def sample_events():
    """Sample event data for testing."""
    return pl.DataFrame({
        "event_id": [1, 2, 2, 3, 4],  # Has duplicate
        "user_id": ["u1", "u2", "u2", None, "u4"],  # Has null
        "event_timestamp": [
            "2024-01-01 10:00:00",
            "2024-01-01 11:00:00",
            "2024-01-01 11:00:00",
            "2024-01-01 12:00:00",
            None,  # Has null
        ],
        "revenue": [10.0, 20.0, 20.0, 30.0, 40.0],
    })

def test_clean_user_events_removes_duplicates(sample_events):
    """Test that duplicates are removed."""
    result = clean_user_events(sample_events)
    assert len(result) == 4  # One duplicate removed

def test_clean_user_events_removes_null_timestamps(sample_events):
    """Test that null timestamps are filtered."""
    result = clean_user_events(sample_events)
    assert result["event_timestamp"].null_count() == 0

def test_clean_user_events_casts_types(sample_events):
    """Test that columns are cast to correct types."""
    result = clean_user_events(sample_events)
    assert result["user_id"].dtype == pl.Utf8
    assert result["event_timestamp"].dtype == pl.Datetime
```

### Integration Testing

```python
# tests/test_integration.py
import polars as pl
from pathlib import Path
import tempfile
import pytest

from my_data_lib import extract_from_csv, clean_user_events, load_to_parquet

@pytest.fixture
def temp_dir():
    """Create temporary directory for test files."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)

def test_full_pipeline(temp_dir):
    """Test complete ETL pipeline."""
    
    # Setup: Create input CSV
    input_file = temp_dir / "input.csv"
    test_data = pl.DataFrame({
        "event_id": [1, 2, 3],
        "user_id": ["u1", "u2", "u3"],
        "event_timestamp": [
            "2024-01-01 10:00:00",
            "2024-01-01 11:00:00",
            "2024-01-01 12:00:00",
        ],
        "revenue": [10.0, 20.0, 30.0],
    })
    test_data.write_csv(input_file)
    
    # Execute pipeline
    df = extract_from_csv(input_file)
    cleaned = clean_user_events(df)
    
    output_file = temp_dir / "output.parquet"
    load_to_parquet(cleaned, output_file)
    
    # Verify
    result = pl.read_parquet(output_file)
    assert len(result) == 3
    assert result["user_id"].dtype == pl.Utf8
```

### Property-Based Testing

```python
from hypothesis import given, strategies as st
import polars as pl

@given(st.lists(st.integers(min_value=1, max_value=1000), min_size=1))
def test_aggregate_sum_is_positive(user_ids):
    """Test that aggregated sums are always positive."""
    df = pl.DataFrame({
        "user_id": user_ids,
        "revenue": [abs(x) for x in user_ids],  # Ensure positive
    })
    
    result = aggregate_metrics(df, ["user_id"])
    assert all(result["total_revenue"] >= 0)
```

---

## Performance Optimization

### Lazy Evaluation

```python
# BAD: Eager evaluation loads everything
df = pl.read_parquet("huge_file.parquet")
filtered = df.filter(pl.col("year") == 2024)
result = filtered.select(["user_id", "revenue"])

# GOOD: Lazy evaluation only loads what's needed
result = (
    pl.scan_parquet("huge_file.parquet")
    .filter(pl.col("year") == 2024)
    .select(["user_id", "revenue"])
    .collect()
)
```

### Columnar Operations

```python
import polars as pl

# BAD: Row-by-row iteration
total = 0
for row in df.iter_rows():
    if row[1] > 100:  # revenue column
        total += row[1]

# GOOD: Vectorized operation
total = df.filter(pl.col("revenue") > 100)["revenue"].sum()
```

### Memory Management

```python
# Stream and process in chunks
def process_in_chunks(filepath, chunk_size=100_000):
    """Process large file without loading all into memory."""
    
    reader = pl.read_csv_batched(filepath, batch_size=chunk_size)
    
    for chunk in reader:
        # Process chunk
        processed = transform_data(chunk)
        
        # Yield results
        yield processed

# Usage
all_results = []
for chunk_result in process_in_chunks("large_file.csv"):
    all_results.append(chunk_result)

final = pl.concat(all_results)
```

### Parallel Processing

```python
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor
import polars as pl

def process_file(filepath: Path) -> pl.DataFrame:
    """Process single file."""
    return pl.read_parquet(filepath).filter(pl.col("year") == 2024)

def process_directory_parallel(dir_path: Path) -> pl.DataFrame:
    """Process all parquet files in directory in parallel."""
    files = list(dir_path.glob("*.parquet"))
    
    with ProcessPoolExecutor() as executor:
        results = executor.map(process_file, files)
    
    return pl.concat(list(results))
```

---

## Common Utilities

### Logging Setup

```python
# src/my_data_lib/logging_config.py
import logging
from pathlib import Path

def setup_logging(
    name: str = "my_data_lib",
    level: int = logging.INFO,
    log_file: Path | None = None
) -> logging.Logger:
    """Configure logging for the library."""
    
    logger = logging.getLogger(name)
    logger.setLevel(level)
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(level)
    
    # Format
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # Optional file handler
    if log_file:
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger

# Usage in your modules
from my_data_lib.logging_config import setup_logging

logger = setup_logging()

def process_data():
    logger.info("Starting data processing")
    # ... processing logic
    logger.info("Processing complete")
```

### Configuration Management

```python
# src/my_data_lib/config.py
from pathlib import Path
from dataclasses import dataclass
import os

@dataclass
class Config:
    """Application configuration."""
    db_connection_string: str
    data_dir: Path
    output_dir: Path
    chunk_size: int = 100_000
    
    @classmethod
    def from_env(cls) -> "Config":
        """Load configuration from environment variables."""
        return cls(
            db_connection_string=os.getenv("DB_CONN", "duckdb://warehouse.db"),
            data_dir=Path(os.getenv("DATA_DIR", "./data")),
            output_dir=Path(os.getenv("OUTPUT_DIR", "./output")),
            chunk_size=int(os.getenv("CHUNK_SIZE", "100000")),
        )

# Usage
config = Config.from_env()
```

### Type Hints

```python
from typing import Protocol, TypeVar, Callable
import polars as pl
from pathlib import Path

# Define DataFrame transformer protocol
class DataFrameTransformer(Protocol):
    """Protocol for DataFrame transformation functions."""
    def __call__(self, df: pl.DataFrame) -> pl.DataFrame: ...

# Generic type for transformers
T = TypeVar('T')

def apply_transformations(
    df: pl.DataFrame,
    transformations: list[DataFrameTransformer]
) -> pl.DataFrame:
    """Apply list of transformations to DataFrame."""
    result = df
    for transform in transformations:
        result = transform(result)
    return result

# Usage with type safety
def remove_nulls(df: pl.DataFrame) -> pl.DataFrame:
    return df.drop_nulls()

def deduplicate(df: pl.DataFrame) -> pl.DataFrame:
    return df.unique()

# These are type-checked!
cleaned = apply_transformations(
    df,
    [remove_nulls, deduplicate]
)
```

---

## Best Practices Summary

1. **Package your code** - Use `pyproject.toml` and proper structure
2. **Test everything** - Unit tests, integration tests, property tests
3. **Use type hints** - Catch errors early, improve documentation
4. **Log appropriately** - Info for progress, debug for troubleshooting
5. **Optimize lazily** - Profile first, optimize second
6. **Choose the right tool** - Standard lib → Polars → DuckDB → Spark
7. **Version your data** - Track schema changes, partition by date
8. **Document** - Docstrings, README, examples

---

## Resources

- [Start Data Engineering](https://www.startdataengineering.com) - Tutorials and best practices
- [Python Packaging Guide](https://packaging.python.org) - Official packaging documentation
- [Polars Documentation](https://pola-rs.github.io/polars/) - Modern DataFrame library
- [DuckDB Documentation](https://duckdb.org) - Analytical database engine
- [Great Expectations](https://greatexpectations.io) - Data quality framework